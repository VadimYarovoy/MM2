Опишем метод решения следующей системы
\[ \vec{A}\vec{x} = \vec{b} \]
где матрица $\vec{A}$ симметричная и положительно определенная.
Возьмем произвольное начальное приближение $\vec{x}^{(0)}$.

\RestyleAlgo{tworuled}
\SetAlFnt{\normalsize}
\SetAlgoNoLine

\subsection{Явный метод}
\begin{algorithm*}[H]
    \DontPrintSemicolon

    $\vec{r}^{(0)} = \vec{b} - \vec{A} \vec{x}^{(0)}$\;
    $\vec{s}^{(1)} = \vec{r}^{(0)}$\;
    $\gamma = \sqrt{\left( \vec{b},\ \vec{b} \right)}$\;

    \For{$k = 1$ \KwTo $k_{max}$}{
        $\displaystyle
            \alpha_k = \frac{\left( \vec{r}^{(k-1)},\ \vec{r}^{(k-1)} \right)}{\left( \vec{A}\vec{s}^{(k)},\ \vec{s}^{(k)} \right)}
        $\;
        $\vec{x}^{(k)} = \vec{x}^{(k-1)} + \alpha_k \vec{s}^{(k)}$\;
        $\vec{r}^{(k)} = \vec{r}^{(k-1)} - \alpha_k \vec{s}^{(k-1)}$\;

        \If{$\sqrt{\left( \vec{r}^{(k)},\ \vec{r}^{(k)} \right)} < \gamma \varepsilon $}{
            \textbf{break}
        }

        $\displaystyle
            \beta_k = \frac{\left( \vec{r}^{(k)},\ \vec{r}^{(k)} \right)}{\left( \vec{r}^{(k-1)},\ \vec{r}^{(k-1)} \right)}
        $\;
        $\vec{s}^{(k+1)} = \vec{r}^{(k)} + \beta_k \vec{s}^{(k)}$\;
    }
\end{algorithm*}

\subsection{Неявный метод}
Неявный метод основан на идее предобуславливания. Выберем матрицу $\vec{B}$, симметричную и положительно определенную, такую, чтобы она была приблизительно равна матрице $\vec{A}$.
Возьмем $\vec{B} = \widetilde{\vec{L}} \widetilde{\vec{L}}^T$, где $\widetilde{\vec{L}} \widetilde{\vec{L}}^T$ -- неполное разложение
Холецкого для матрицы $\vec{A}$. Позже объясним, почему такой выбор матрицы $\vec{B}$ является удачным.

\begin{algorithm*}[H]
    \DontPrintSemicolon

    $\vec{r}^{(0)} = \vec{b} - \vec{A} \vec{x}^{(0)}$\;
    $\vec{B} \vec{w}^{(0)} = \vec{r}^{(0)}$\;
    $\vec{s}^{(1)} = \vec{w}^{(0)}$\;
    $\vec{B} \vec{g} = \vec{b}$\;
    $\gamma = \sqrt{\left( \vec{g},\ \vec{b} \right)}$\;

    \For{$k = 1$ \KwTo $k_{max}$}{
        $\displaystyle
            \alpha_k = \frac{\left( \vec{w}^{(k-1)},\ \vec{r}^{(k-1)} \right)}{\left( \vec{A}\vec{s}^{(k)},\ \vec{s}^{(k)} \right)}
        $\;
        $\vec{x}^{(k)} = \vec{x}^{(k-1)} + \alpha_k \vec{s}^{(k)}$\;
        $\vec{r}^{(k)} = \vec{r}^{(k-1)} - \alpha_k \vec{s}^{(k-1)}$\;
        $\vec{B}\vec{w}^{(k)} = \vec{r}^{(k)}$\;

        \If{$\sqrt{\left( \vec{w}^{(k)},\ \vec{r}^{(k)} \right)} < \gamma \varepsilon $}{
            \textbf{break}
        }

        $\displaystyle
            \beta_k = \frac{\left( \vec{w}^{(k)},\ \vec{r}^{(k)} \right)}{\left( \vec{w}^{(k-1)},\ \vec{r}^{(k-1)} \right)}
        $\;
        $\vec{s}^{(k+1)} = \vec{w}^{(k)} + \beta_k \vec{s}^{(k)}$\;
    }
\end{algorithm*}
\noindent Систему $\vec{B} \vec{w}^{(k)} = \vec{r}^{(k)}$ будем решать методом Гаусса. Выбирая $\vec{B} = \widetilde{\vec{L}}\widetilde{\vec{L}}^T$, остается только сделать два раза обратную
подстановку: $\widetilde{\vec{L}} \vec{z}^{(k)} = \vec{r}^{(k)}$ и $\widetilde{\vec{L}}^T \vec{w}^{(k)} = \vec{z}^{(k)}$.

\newpage
\begin{landscape}
\subsection{Пример работы алгоритма}
Рассмотрим следующий численный пример.
\begin{align*}
    \vec{A} &=
    \begin{pmatrix}
        3.56 & 1.27 & 0 & 1.20 & 0 & 0 & 0 & 0 & 0 \\
        1.27 & 5.82 & 1.58 & 0 & 1.81 & 0 & 0 & 0 & 0 \\
        0 & 1.58 & 3.31 & 1.33 & 0 & 1.83 & 0 & 0 & 0 \\
        1.20 & 0 & 1.33 & 5.08 & 1.20 & 0 & 1.87 & 0 & 0 \\
        0 & 1.81 & 0 & 1.20 & 5.67 & 1.94 & 0 & 1.24 & 0 \\
        0 & 0 & 1.83 & 0 & 1.94 & 3.44 & 1.78 & 0 & 1.25 \\
        0 & 0 & 0 & 1.87 & 0 & 1.78 & 4.03 & 1.70 & 0 \\
        0 & 0 & 0 & 0 & 1.24 & 0 & 1.70 & 5.29 & 1.64 \\
        0 & 0 & 0 & 0 & 0 & 1.25 & 0 & 1.64 & 5.64 \\
    \end{pmatrix}
    &
    \vec{x} &= \begin{pmatrix}
        1.95 \\ 1.98 \\ 1.88 \\ 1.41 \\ 1.37 \\ 1.16 \\ 1.99 \\ 1.46 \\ 1.94 \\
    \end{pmatrix}
    &
    \vec{b} &= \begin{pmatrix}
        11.17 \\ 19.44 \\ 13.34 \\ 17.37 \\ 17.11 \\ 16.07 \\ 15.21 \\ 16.02 \\ 14.81 \\
    \end{pmatrix} \\
\end{align*}
Задаимся нулевым начальным приближением и значением $\varepsilon = 0.01$. Обозначим сделанное количество итераций как $n$,
погрешность решения будем вычислять следующим образом.
\[
    \delta = \norm{\vec{x} - \tilde{\vec{x}}}_2
\]
Полученные решения для явного и неявного методов, соответственно, приведены ниже.
\[
    \tilde{\vec{x}}^T = \begin{pmatrix}
        1.92 & 2.01 & 1.85 & 1.39 & 1.36 & 1.14 & 2.05 & 1.46 & 1.95 \\
    \end{pmatrix},\quad n = 5,\quad \delta = 8.13 \cdot 10^{-2}
\]
\[
    \tilde{\vec{x}}^T = \begin{pmatrix}
        1.95 & 1.96 & 1.89 & 1.41 & 1.39 & 1.17 & 1.98 & 1.46 & 1.94 \\
    \end{pmatrix},\quad n = 4,\quad \delta = 2.97 \cdot 10^{-2}
\]
\end{landscape}

\newpage
\begin{landscape}
Также приведем разложение Холецкого для данного примера.
\[
    \widetilde{\vec{L}} =
    \begin{pmatrix}
        1.89 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0.67 & 2.32 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0.68 & 1.69 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0.64 & 0 & 0.79 & 2.01 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0.78 & 0 & 0.60 & 2.17 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1.08 & 0 & 0.89 & 1.21 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0.93 & 0 & 1.47 & 1.00 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0.57 & 0 & 1.70 & 1.44 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1.03 & 0 & 1.14 & 1.81 \\
    \end{pmatrix}
\]
\[
    \widetilde{\vec{L}} \widetilde{\vec{L}}^T =
    \begin{pmatrix}
        3.56 & 1.27 & 0.0 & 1.20 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
        1.27 & 5.82 & 1.58 & 0.43 & 1.81 & 0.0 & 0.0 & 0.0 & 0.0 \\
        0.0 & 1.58 & 3.31 & 1.33 & 0.53 & 1.83 & 0.0 & 0.0 & 0.0 \\
        1.20 & 0.43 & 1.33 & 5.08 & 1.20 & 0.86 & 1.87 & 0.0 & 0.0 \\
        0.0 & 1.81 & 0.53 & 1.20 & 5.67 & 1.94 & 0.55 & 1.24 & 0.0 \\
        0.0 & 0.0 & 1.83 & 0.86 & 1.94 & 3.44 & 1.78 & 0.51 & 1.25 \\
        0.0 & 0.0 & 0.0 & 1.87 & 0.55 & 1.78 & 4.03 & 1.70 & 1.52 \\
        0.0 & 0.0 & 0.0 & 0.0 & 1.24 & 0.51 & 1.70 & 5.29 & 1.64 \\
        0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 1.25 & 1.52 & 1.64 & 5.64 \\
    \end{pmatrix}
\]
\end{landscape}

Знаем, что матрица в диагональным преобладанием не будет плохо обусловленной.
Для того, чтобы продемонстрировать разную скорость сходимости двух методом,
выберем для тестов матрицу без явного диагонального преобладания. Матрицу заполним
случайными числами, размер матрицы -- 100. Зададим значение $\varepsilon = 10^{-5}$.
\begin{table}[H]
    \centering
    \begin{tabular}{*5c}
        \toprule
        \multirow[c]{2}{*}{\makecell{№\\теста}} & \multicolumn{2}{c}{Явный метод} & \multicolumn{2}{c}{Неявный метод} \\
        \cmidrule{2-5}
        & $n$ & $\delta$ & $n$ & $\delta$ \\
        \midrule
        1 & 38 & $1.87 \cdot 10^{-3}$ & 14 & $2.75 \cdot 10^{-4}$ \\
        2 & 37 & $4.92 \cdot 10^{-4}$ & 14 & $1.68 \cdot 10^{-4}$ \\
        3 & 35 & $4.12 \cdot 10^{-3}$ & 13 & $5.41 \cdot 10^{-4}$ \\
        \bottomrule
    \end{tabular}
\end{table}
\noindent Видим, что неявный метод сходится быстрее.